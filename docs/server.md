# OpenAI-Compatible Server

DeepRecall can run as an OpenAI-compatible REST API, so any tool that speaks the OpenAI protocol (ChatGPT plugins, LangChain, etc.) can use it as a drop-in backend.

## Quick Start

```bash
pip install deeprecall[server,chroma]

deeprecall serve --vectorstore chroma --collection my_docs --port 8000
```

Then query it like OpenAI:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "What are the key themes?"}]}'
```

---

## Endpoints

### `POST /v1/chat/completions`

OpenAI-compatible chat completion. Supports both regular and streaming responses.

**Request:**

```json
{
  "model": "deeprecall",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What caused the 2008 crisis?"}
  ],
  "stream": false,
  "temperature": 0.7
}
```

**Response:**

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "model": "deeprecall",
  "choices": [{"index": 0, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"}],
  "usage": {"prompt_tokens": 1200, "completion_tokens": 350, "total_tokens": 1550}
}
```

When `"stream": true`, returns Server-Sent Events in OpenAI streaming format.

### `POST /v1/documents`

Add documents to the vector store (DeepRecall extension -- not in OpenAI spec).

**Request:**

```json
{
  "documents": ["Document 1 text...", "Document 2 text..."],
  "metadatas": [{"source": "a.txt"}, {"source": "b.txt"}],
  "ids": ["doc-1", "doc-2"]
}
```

**Response:**

```json
{"ids": ["doc-1", "doc-2"], "count": 2}
```

### `GET /v1/models`

List available models.

### `GET /v1/usage`

Return cumulative usage stats (if `UsageTrackingCallback` is registered).

### `POST /v1/cache/clear`

Clear the query cache. Non-blocking -- runs in thread pool.

### `GET /health`

Health check endpoint. Returns `{"status": "ok"}`.

### `GET /docs`

Interactive Swagger UI (auto-generated by FastAPI).

---

## Authentication

Add API key authentication with `--api-keys`:

```bash
deeprecall serve --api-keys "sk-key1,sk-key2"
```

Clients must include a Bearer token:

```bash
curl -H "Authorization: Bearer sk-key1" http://localhost:8000/v1/chat/completions ...
```

### Programmatic Setup

```python
from deeprecall.adapters.openai_server import create_app
from deeprecall.middleware.auth import APIKeyAuth

app = create_app(engine, api_keys=["sk-key1", "sk-key2"])
```

### Custom Validator

Support both sync and async validation functions (e.g. database lookups):

```python
# Sync validator (runs in thread pool automatically)
def validate_in_db(key: str) -> bool:
    return db.api_keys.exists(key)

app.add_middleware(APIKeyAuth, validate_fn=validate_in_db)

# Async validator
async def validate_async(key: str) -> bool:
    return await db.api_keys.exists(key)

app.add_middleware(APIKeyAuth, validate_fn=validate_async)
```

### APIKeyAuth Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `api_keys` | `list[str] \| None` | `None` | Valid API keys. `None` disables auth |
| `validate_fn` | `Callable \| None` | `None` | Custom sync or async validator |
| `exempt_paths` | `list[str]` | `["/health", "/docs", "/openapi.json"]` | Paths that skip auth |

---

## Rate Limiting

Add per-key rate limiting with `--rate-limit`:

```bash
deeprecall serve --rate-limit 60  # 60 requests/min per key
```

Uses a token bucket algorithm. Thread-safe under concurrent access.

### RateLimiter Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `requests_per_minute` | `int` | `60` | Max requests per minute per API key or IP |
| `exempt_paths` | `list[str]` | `["/health", "/docs", "/openapi.json"]` | Paths that skip rate limiting |

When rate limited, returns HTTP 429 with a `Retry-After` header:

```json
{"error": {"message": "Rate limit exceeded", "type": "rate_limit_error", "retry_after": 1.5}}
```

---

## Concurrency Model

All blocking operations run in a thread pool via `asyncio.to_thread()`:

- `POST /v1/chat/completions` -- `engine.query()` in thread pool
- `POST /v1/documents` -- `engine.add_documents()` in thread pool
- `POST /v1/cache/clear` -- `cache.clear()` in thread pool

This means the server can handle many concurrent HTTP requests without blocking.
